{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqobkNUpPKCv"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession, Window\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import types as T\n",
        "from datetime import datetime\n",
        "import json\n",
        "import hashlib\n",
        "\n",
        "class F1SilverLayer:\n",
        "    def __init__(self,\n",
        "                 spark,\n",
        "                 bronze_path=\"/content/sample_data/bronze/\",\n",
        "                 silver_path=\"/content/sample_data/silver/\",\n",
        "                 checkpoint_path=\"/content/sample_data/silver_checkpoint/\"):\n",
        "        \"\"\"\n",
        "        Initialize the F1 Silver Layer processor\n",
        "        \"\"\"\n",
        "        self.spark = spark\n",
        "        self.bronze_path = bronze_path\n",
        "        self.silver_path = silver_path\n",
        "        self.checkpoint_path = checkpoint_path\n",
        "\n",
        "        # Configure Spark for optimal performance\n",
        "        self.spark.conf.set(\"spark.sql.parquet.compression.codec\", \"zstd\")\n",
        "        self.spark.conf.set(\"spark.sql.parquet.enableDictionaryEncoding\", \"true\")\n",
        "        self.spark.conf.set(\"spark.sql.parquet.block.size\", 256*1024*1024)\n",
        "\n",
        "        # Define schema for better control and performance\n",
        "        self.define_schemas()\n",
        "\n",
        "    def define_schemas(self):\n",
        "        \"\"\"Define explicit schemas for the silver layer\"\"\"\n",
        "        self.circuit_schema = T.StructType([\n",
        "            T.StructField(\"circuitId\", T.StringType(), True),\n",
        "            T.StructField(\"circuitName\", T.StringType(), True),\n",
        "            T.StructField(\"lat\", T.DoubleType(), True),\n",
        "            T.StructField(\"long\", T.DoubleType(), True),\n",
        "            T.StructField(\"locality\", T.StringType(), True),\n",
        "            T.StructField(\"country\", T.StringType(), True)\n",
        "        ])\n",
        "\n",
        "        self.result_schema = T.StructType([\n",
        "            T.StructField(\"constructorId\", T.StringType(), True),\n",
        "            T.StructField(\"constructorName\", T.StringType(), True),\n",
        "            T.StructField(\"driverId\", T.StringType(), True),\n",
        "            T.StructField(\"driverName\", T.StringType(), True),\n",
        "            T.StructField(\"position\", T.IntegerType(), True),\n",
        "            T.StructField(\"points\", T.DoubleType(), True),\n",
        "            T.StructField(\"grid\", T.IntegerType(), True),\n",
        "            T.StructField(\"laps\", T.IntegerType(), True),\n",
        "            T.StructField(\"status\", T.StringType(), True),\n",
        "            T.StructField(\"time\", T.StringType(), True)\n",
        "        ])\n",
        "\n",
        "    def get_last_processed_timestamp(self):\n",
        "        \"\"\"Get the last processed timestamp from checkpoint\"\"\"\n",
        "        try:\n",
        "            checkpoint_df = self.spark.read.parquet(self.checkpoint_path)\n",
        "            last_checkpoint = checkpoint_df.orderBy(F.col(\"processed_timestamp\").desc()).first()\n",
        "            return last_checkpoint.processed_timestamp\n",
        "        except:\n",
        "            return \"1900-01-01T00:00:00\"\n",
        "\n",
        "    def process_bronze_data(self):\n",
        "        \"\"\"Process bronze data incrementally\"\"\"\n",
        "        # Read bronze data\n",
        "        bronze_df = self.spark.read.json(f\"{self.bronze_path}/season=*\")\n",
        "\n",
        "        # Get last processed timestamp\n",
        "        last_processed = self.get_last_processed_timestamp()\n",
        "\n",
        "        # Filter for new records\n",
        "        incremental_df = bronze_df.filter(\n",
        "            F.col(\"ingestion_timestamp\") > last_processed\n",
        "        )\n",
        "\n",
        "        if incremental_df.count() == 0:\n",
        "            print(\"No new data to process\")\n",
        "            return None\n",
        "\n",
        "        return self.transform_bronze_to_silver(incremental_df)\n",
        "\n",
        "    def transform_bronze_to_silver(self, df):\n",
        "        \"\"\"Transform bronze data into silver format with quality checks\"\"\"\n",
        "\n",
        "        # 1. Explode nested structures\n",
        "        df = df.select(\n",
        "            F.col(\"season\"),\n",
        "            F.col(\"round\"),\n",
        "            F.col(\"raceName\"),\n",
        "            F.col(\"date\"),\n",
        "            F.col(\"time\"),\n",
        "            F.to_timestamp(F.col(\"ingestion_timestamp\")).alias(\"ingestion_timestamp\"),\n",
        "            F.col(\"url\"),\n",
        "            F.col(\"Circuit\").alias(\"circuit\"),\n",
        "            F.explode(\"Results\").alias(\"result\")\n",
        "        )\n",
        "\n",
        "        # 2. Flatten nested structures\n",
        "        df = df.select(\n",
        "            \"*\",\n",
        "            F.col(\"circuit.circuitId\").alias(\"circuit_id\"),\n",
        "            F.col(\"circuit.circuitName\").alias(\"circuit_name\"),\n",
        "            F.col(\"circuit.Location.lat\").alias(\"circuit_lat\"),\n",
        "            F.col(\"circuit.Location.long\").alias(\"circuit_long\"),\n",
        "            F.col(\"circuit.Location.locality\").alias(\"circuit_locality\"),\n",
        "            F.col(\"circuit.Location.country\").alias(\"circuit_country\"),\n",
        "            F.col(\"result.Constructor.constructorId\").alias(\"constructor_id\"),\n",
        "            F.col(\"result.Constructor.name\").alias(\"constructor_name\"),\n",
        "            F.col(\"result.Driver.driverId\").alias(\"driver_id\"),\n",
        "            F.col(\"result.Driver.givenName\").alias(\"driver_given_name\"),\n",
        "            F.col(\"result.Driver.familyName\").alias(\"driver_family_name\"),\n",
        "            F.col(\"result.position\").alias(\"position\"),\n",
        "            F.col(\"result.points\").alias(\"points\"),\n",
        "            F.col(\"result.grid\").alias(\"grid\"),\n",
        "            F.col(\"result.laps\").alias(\"laps\"),\n",
        "            F.col(\"result.status\").alias(\"status\"),\n",
        "            F.col(\"result.Time.time\").alias(\"finish_time\")\n",
        "        ).drop(\"circuit\", \"result\")\n",
        "\n",
        "        # 3. Data type conversions and standardization\n",
        "        df = df.withColumn(\"race_timestamp\",\n",
        "                          F.to_timestamp(\n",
        "                              F.concat(F.col(\"date\"), F.lit(\" \"), F.col(\"time\")),\n",
        "                              \"yyyy-MM-dd HH:mm:ssX\"\n",
        "                          ))\n",
        "\n",
        "        # 4. Add computed columns\n",
        "        df = df.withColumn(\"driver_full_name\",\n",
        "                          F.concat(F.col(\"driver_given_name\"), F.lit(\" \"), F.col(\"driver_family_name\")))\n",
        "\n",
        "        # 5. Generate hash key for change detection\n",
        "        columns_for_hash = [\"season\", \"round\", \"driver_id\", \"constructor_id\", \"position\"]\n",
        "        df = df.withColumn(\"row_hash\",\n",
        "                          F.sha2(F.concat_ws(\"|\", *[F.col(c) for c in columns_for_hash]), 256))\n",
        "\n",
        "        # 6. Add data quality columns\n",
        "        df = self.add_data_quality_checks(df)\n",
        "\n",
        "        # 7. Add metadata columns\n",
        "        df = df.withColumn(\"processed_timestamp\", F.current_timestamp())\n",
        "        df = df.withColumn(\"silver_batch_id\", F.uuid())\n",
        "\n",
        "        return df\n",
        "\n",
        "    def add_data_quality_checks(self, df):\n",
        "        \"\"\"Add data quality check columns\"\"\"\n",
        "\n",
        "        # Define quality checks\n",
        "        df = df.withColumn(\"is_valid_position\",\n",
        "                          (F.col(\"position\").isNotNull() & (F.col(\"position\") >= 1)))\n",
        "\n",
        "        df = df.withColumn(\"is_valid_points\",\n",
        "                          (F.col(\"points\").isNotNull() & (F.col(\"points\") >= 0)))\n",
        "\n",
        "        df = df.withColumn(\"is_valid_grid\",\n",
        "                          (F.col(\"grid\").isNotNull() & (F.col(\"grid\") >= 0)))\n",
        "\n",
        "        df = df.withColumn(\"is_valid_date\",\n",
        "                          F.col(\"race_timestamp\").isNotNull())\n",
        "\n",
        "        # Combine all checks\n",
        "        df = df.withColumn(\"is_valid_record\",\n",
        "                          F.col(\"is_valid_position\") &\n",
        "                          F.col(\"is_valid_points\") &\n",
        "                          F.col(\"is_valid_grid\") &\n",
        "                          F.col(\"is_valid_date\"))\n",
        "\n",
        "        # Calculate null percentages\n",
        "        for column in df.columns:\n",
        "            df = df.withColumn(f\"is_null_{column}\",\n",
        "                             F.when(F.col(column).isNull(), 1).otherwise(0))\n",
        "\n",
        "        return df\n",
        "\n",
        "    def write_to_silver(self, df):\n",
        "        \"\"\"Write processed data to silver layer\"\"\"\n",
        "        if df is None or df.count() == 0:\n",
        "            return\n",
        "\n",
        "        # Write main silver table\n",
        "        (df.write\n",
        "         .mode(\"append\")\n",
        "         .partitionBy(\"season\")\n",
        "         .format(\"parquet\")\n",
        "         .option(\"compression\", \"zstd\")\n",
        "         .save(self.silver_path))\n",
        "\n",
        "        # Write quality metrics\n",
        "        quality_metrics = self.calculate_quality_metrics(df)\n",
        "        (quality_metrics.write\n",
        "         .mode(\"append\")\n",
        "         .format(\"parquet\")\n",
        "         .save(f\"{self.silver_path}_metrics\"))\n",
        "\n",
        "        # Update checkpoint\n",
        "        self.update_checkpoint(df)\n",
        "\n",
        "    def calculate_quality_metrics(self, df):\n",
        "        \"\"\"Calculate quality metrics for the batch\"\"\"\n",
        "        metrics = []\n",
        "\n",
        "        # Calculate null percentages\n",
        "        for column in df.columns:\n",
        "            null_count = df.filter(F.col(column).isNull()).count()\n",
        "            total_count = df.count()\n",
        "            null_percentage = (null_count / total_count) * 100 if total_count > 0 else 0\n",
        "            metrics.append({\n",
        "                \"metric_name\": f\"null_percentage_{column}\",\n",
        "                \"metric_value\": null_percentage,\n",
        "                \"batch_id\": df.select(F.first(\"silver_batch_id\")).collect()[0][0],\n",
        "                \"calculated_at\": datetime.now().isoformat()\n",
        "            })\n",
        "\n",
        "        # Add other metrics\n",
        "        metrics.extend([\n",
        "            {\n",
        "                \"metric_name\": \"total_records\",\n",
        "                \"metric_value\": df.count(),\n",
        "                \"batch_id\": df.select(F.first(\"silver_batch_id\")).collect()[0][0],\n",
        "                \"calculated_at\": datetime.now().isoformat()\n",
        "            },\n",
        "            {\n",
        "                \"metric_name\": \"invalid_records_percentage\",\n",
        "                \"metric_value\": (df.filter(~F.col(\"is_valid_record\")).count() / df.count()) * 100,\n",
        "                \"batch_id\": df.select(F.first(\"silver_batch_id\")).collect()[0][0],\n",
        "                \"calculated_at\": datetime.now().isoformat()\n",
        "            }\n",
        "        ])\n",
        "\n",
        "        return self.spark.createDataFrame(metrics)\n",
        "\n",
        "    def update_checkpoint(self, df):\n",
        "        \"\"\"Update the checkpoint with latest processed timestamp\"\"\"\n",
        "        checkpoint_data = [{\n",
        "            \"processed_timestamp\": df.select(F.max(\"ingestion_timestamp\")).collect()[0][0],\n",
        "            \"silver_batch_id\": df.select(F.first(\"silver_batch_id\")).collect()[0][0],\n",
        "            \"record_count\": df.count(),\n",
        "            \"checkpoint_timestamp\": datetime.now().isoformat()\n",
        "        }]\n",
        "\n",
        "        (self.spark.createDataFrame(checkpoint_data)\n",
        "         .write\n",
        "         .mode(\"append\")\n",
        "         .parquet(self.checkpoint_path))\n",
        "\n",
        "    def process(self):\n",
        "        \"\"\"Main processing method\"\"\"\n",
        "        try:\n",
        "            print(\"Starting Silver Layer Processing...\")\n",
        "\n",
        "            # Process bronze to silver\n",
        "            silver_df = self.process_bronze_data()\n",
        "\n",
        "            # Write to silver if we have data\n",
        "            if silver_df is not None:\n",
        "                print(f\"Writing {silver_df.count()} records to silver layer...\")\n",
        "                self.write_to_silver(silver_df)\n",
        "                print(\"Silver Layer Processing Complete!\")\n",
        "            else:\n",
        "                print(\"No new data to process\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in Silver Layer Processing: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "# Usage\n",
        "if __name__ == \"__main__\":\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"F1SilverLayer\") \\\n",
        "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    silver_layer = F1SilverLayer(spark)\n",
        "    silver_layer.process()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession, Window\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import types as T\n",
        "from datetime import datetime\n",
        "import json\n",
        "import hashlib\n",
        "\n",
        "class F1SilverLayer:\n",
        "    def __init__(self,\n",
        "                 spark,\n",
        "                 bronze_path=\"/content/sample_data/bronze/\",\n",
        "                 silver_path=\"/content/sample_data/silver/\",\n",
        "                 checkpoint_path=\"/content/sample_data/silver_checkpoint/\"):\n",
        "        \"\"\"\n",
        "        Initialize the F1 Silver Layer processor\n",
        "        \"\"\"\n",
        "        self.spark = spark\n",
        "        self.bronze_path = bronze_path\n",
        "        self.silver_path = silver_path\n",
        "        self.checkpoint_path = checkpoint_path\n",
        "\n",
        "        # Configure Spark for optimal performance\n",
        "        self.spark.conf.set(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
        "        self.spark.conf.set(\"spark.sql.parquet.enableDictionaryEncoding\", \"true\")\n",
        "        self.spark.conf.set(\"spark.sql.parquet.block.size\", 256*1024*1024)\n",
        "\n",
        "    def get_last_processed_timestamp(self):\n",
        "        \"\"\"Get the last processed timestamp from checkpoint\"\"\"\n",
        "        try:\n",
        "            checkpoint_df = self.spark.read.parquet(self.checkpoint_path)\n",
        "            last_checkpoint = checkpoint_df.orderBy(F.col(\"processed_timestamp\").desc()).first()\n",
        "            return last_checkpoint.processed_timestamp\n",
        "        except:\n",
        "            return \"1900-01-01T00:00:00\"\n",
        "\n",
        "    def process_bronze_data(self):\n",
        "        \"\"\"Process bronze data incrementally\"\"\"\n",
        "        # Read bronze data\n",
        "        bronze_df = spark.read.json('/content/sample_data/bronze/season=*')\n",
        "\n",
        "        # Get last processed timestamp\n",
        "        last_processed = self.get_last_processed_timestamp()\n",
        "\n",
        "        # Filter for new records\n",
        "        incremental_df = bronze_df.filter(\n",
        "            F.col(\"ingestion_timestamp\") > last_processed\n",
        "        )\n",
        "\n",
        "        if incremental_df.count() == 0:\n",
        "            print(\"No new data to process\")\n",
        "            return None\n",
        "\n",
        "        return self.transform_bronze_to_silver(incremental_df)\n",
        "\n",
        "    def transform_bronze_to_silver(self, df):\n",
        "        \"\"\"Transform bronze data into silver format with quality checks\"\"\"\n",
        "\n",
        "        # 1. Explode nested structures\n",
        "        df = df.select(\n",
        "            F.col(\"season\"),\n",
        "            F.col(\"round\"),\n",
        "            F.col(\"raceName\"),\n",
        "            F.col(\"date\"),\n",
        "            F.col(\"time\"),\n",
        "            F.to_timestamp(F.col(\"ingestion_timestamp\")).alias(\"ingestion_timestamp\"),\n",
        "            F.col(\"url\"),\n",
        "            F.col(\"Circuit\").alias(\"circuit\"),\n",
        "            F.explode(\"Results\").alias(\"result\")\n",
        "        )\n",
        "\n",
        "        # 2. Flatten nested structures\n",
        "        df = df.select(\n",
        "            \"*\",\n",
        "            F.col(\"circuit.circuitId\").alias(\"circuit_id\"),\n",
        "            F.col(\"circuit.circuitName\").alias(\"circuit_name\"),\n",
        "            F.col(\"circuit.Location.lat\").alias(\"circuit_lat\"),\n",
        "            F.col(\"circuit.Location.long\").alias(\"circuit_long\"),\n",
        "            F.col(\"circuit.Location.locality\").alias(\"circuit_locality\"),\n",
        "            F.col(\"circuit.Location.country\").alias(\"circuit_country\"),\n",
        "            F.col(\"result.Constructor.constructorId\").alias(\"constructor_id\"),\n",
        "            F.col(\"result.Constructor.name\").alias(\"constructor_name\"),\n",
        "            F.col(\"result.Driver.driverId\").alias(\"driver_id\"),\n",
        "            F.col(\"result.Driver.givenName\").alias(\"driver_given_name\"),\n",
        "            F.col(\"result.Driver.familyName\").alias(\"driver_family_name\"),\n",
        "            F.col(\"result.position\").alias(\"position\"),\n",
        "            F.col(\"result.points\").alias(\"points\"),\n",
        "            F.col(\"result.grid\").alias(\"grid\"),\n",
        "            F.col(\"result.laps\").alias(\"laps\"),\n",
        "            F.col(\"result.status\").alias(\"status\"),\n",
        "            F.col(\"result.Time.time\").alias(\"finish_time\")\n",
        "        ).drop(\"circuit\", \"result\")\n",
        "\n",
        "        # 3. Data type conversions and standardization\n",
        "        df = df.withColumn(\"race_timestamp\",\n",
        "                          F.to_timestamp(\n",
        "                              F.concat(F.col(\"date\"), F.lit(\" \"), F.col(\"time\")),\n",
        "                              \"yyyy-MM-dd HH:mm:ssX\"\n",
        "                          ))\n",
        "\n",
        "        # 4. Add computed columns\n",
        "        df = df.withColumn(\"driver_full_name\",\n",
        "                          F.concat(F.col(\"driver_given_name\"), F.lit(\" \"), F.col(\"driver_family_name\")))\n",
        "\n",
        "        # 5. Generate hash key for change detection\n",
        "        columns_for_hash = [\"season\", \"round\", \"driver_id\", \"constructor_id\", \"position\"]\n",
        "        df = df.withColumn(\"row_hash\",\n",
        "                          F.sha2(F.concat_ws(\"|\", *[F.col(c) for c in columns_for_hash]), 256))\n",
        "\n",
        "        # 6. Add data quality columns\n",
        "        df = self.add_data_quality_checks(df)\n",
        "\n",
        "        # 7. Add metadata columns\n",
        "        df = df.withColumn(\"processed_timestamp\", F.current_timestamp())\n",
        "        df = df.withColumn(\"silver_batch_id\", F.expr(\"uuid()\"))\n",
        "\n",
        "        return df\n",
        "\n",
        "    def add_data_quality_checks(self, df):\n",
        "        \"\"\"Add data quality check columns\"\"\"\n",
        "        # Define quality checks\n",
        "        df = df.withColumn(\"is_valid_position\",\n",
        "                          (F.col(\"position\").isNotNull() & (F.col(\"position\") >= 1)))\n",
        "\n",
        "        df = df.withColumn(\"is_valid_points\",\n",
        "                          (F.col(\"points\").isNotNull() & (F.col(\"points\") >= 0)))\n",
        "\n",
        "        df = df.withColumn(\"is_valid_grid\",\n",
        "                          (F.col(\"grid\").isNotNull() & (F.col(\"grid\") >= 0)))\n",
        "\n",
        "        df = df.withColumn(\"is_valid_date\",\n",
        "                          F.col(\"race_timestamp\").isNotNull())\n",
        "\n",
        "        # Combine all checks\n",
        "        df = df.withColumn(\"is_valid_record\",\n",
        "                          F.col(\"is_valid_position\") &\n",
        "                          F.col(\"is_valid_points\") &\n",
        "                          F.col(\"is_valid_grid\") &\n",
        "                          F.col(\"is_valid_date\"))\n",
        "\n",
        "        return df\n",
        "\n",
        "    def write_to_silver(self, df):\n",
        "        \"\"\"Write processed data to silver layer\"\"\"\n",
        "        if df is None or df.count() == 0:\n",
        "            return\n",
        "\n",
        "        # Write main silver table\n",
        "        (df.write\n",
        "         .mode(\"append\")\n",
        "         .partitionBy(\"season\")\n",
        "         .format(\"parquet\")\n",
        "         .save(self.silver_path))\n",
        "\n",
        "        # Write quality metrics\n",
        "        quality_metrics = self.calculate_quality_metrics(df)\n",
        "        (quality_metrics.write\n",
        "         .mode(\"append\")\n",
        "         .format(\"parquet\")\n",
        "         .save(f\"{self.silver_path}_metrics\"))\n",
        "\n",
        "        # Update checkpoint\n",
        "        self.update_checkpoint(df)\n",
        "\n",
        "    def calculate_quality_metrics(self, df):\n",
        "        \"\"\"Calculate quality metrics for the batch\"\"\"\n",
        "        metrics = []\n",
        "\n",
        "        # Calculate null percentages\n",
        "        for column in df.columns:\n",
        "            null_count = df.filter(F.col(column).isNull()).count()\n",
        "            total_count = df.count()\n",
        "            null_percentage = (null_count / total_count) * 100 if total_count > 0 else 0\n",
        "            metrics.append({\n",
        "                \"metric_name\": f\"null_percentage_{column}\",\n",
        "                \"metric_value\": null_percentage,\n",
        "                \"batch_id\": df.select(F.first(\"silver_batch_id\")).collect()[0][0],\n",
        "                \"calculated_at\": datetime.now().isoformat()\n",
        "            })\n",
        "\n",
        "        # Add other metrics\n",
        "        metrics.extend([\n",
        "            {\n",
        "                \"metric_name\": \"total_records\",\n",
        "                \"metric_value\": df.count(),\n",
        "                \"batch_id\": df.select(F.first(\"silver_batch_id\")).collect()[0][0],\n",
        "                \"calculated_at\": datetime.now().isoformat()\n",
        "            },\n",
        "            {\n",
        "                \"metric_name\": \"invalid_records_percentage\",\n",
        "                \"metric_value\": (df.filter(~F.col(\"is_valid_record\")).count() / df.count()) * 100,\n",
        "                \"batch_id\": df.select(F.first(\"silver_batch_id\")).collect()[0][0],\n",
        "                \"calculated_at\": datetime.now().isoformat()\n",
        "            }\n",
        "        ])\n",
        "\n",
        "        return self.spark.createDataFrame(metrics)\n",
        "\n",
        "    def update_checkpoint(self, df):\n",
        "        \"\"\"Update the checkpoint with latest processed timestamp\"\"\"\n",
        "        checkpoint_data = [{\n",
        "            \"processed_timestamp\": df.select(F.max(\"ingestion_timestamp\")).collect()[0][0],\n",
        "            \"silver_batch_id\": df.select(F.first(\"silver_batch_id\")).collect()[0][0],\n",
        "            \"record_count\": df.count(),\n",
        "            \"checkpoint_timestamp\": datetime.now().isoformat()\n",
        "        }]\n",
        "\n",
        "        (self.spark.createDataFrame(checkpoint_data)\n",
        "         .write\n",
        "         .mode(\"append\")\n",
        "         .parquet(self.checkpoint_path))\n",
        "\n",
        "    def process(self):\n",
        "        \"\"\"Main processing method\"\"\"\n",
        "        try:\n",
        "            print(\"Starting Silver Layer Processing...\")\n",
        "\n",
        "            # Process bronze to silver\n",
        "            silver_df = self.process_bronze_data()\n",
        "\n",
        "            # Write to silver if we have data\n",
        "            if silver_df is not None:\n",
        "                print(f\"Writing {silver_df.count()} records to silver layer...\")\n",
        "                self.write_to_silver(silver_df)\n",
        "                print(\"Silver Layer Processing Complete!\")\n",
        "            else:\n",
        "                print(\"No new data to process\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in Silver Layer Processing: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "# Initialize Spark Session (simplified for Colab)\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"F1SilverLayer\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Create and run silver layer\n",
        "silver_layer = F1SilverLayer(\n",
        "    spark,\n",
        "    bronze_path=\"/content/sample_data/bronze/\",\n",
        "    silver_path=\"/content/sample_data/silver/\",\n",
        "    checkpoint_path=\"/content/sample_data/silver_checkpoint/\"\n",
        ")\n",
        "\n",
        "# Process data\n",
        "silver_layer.process()"
      ],
      "metadata": {
        "id": "-aletruSR8Im"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}