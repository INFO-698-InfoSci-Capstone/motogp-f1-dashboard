{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbUG3DLVKlPT",
        "outputId": "0e16bd55-9d06-4bdb-acdd-6c1f7d8821e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last processed season from checkpoint: 2025\n",
            "Added current season 2025 to processing list\n",
            "Processing seasons: [2025]\n",
            "\n",
            "Processing season 2025\n",
            "Force refreshing current season 2025\n",
            "Deleting existing data for season 2025\n",
            "Fetching season 2025\n",
            "Saved 4 races for season 2025\n",
            "Bronze layer update complete\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2025]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "def fetch_f1_bronze_layer_incremental(spark, bronze_path=\"/content/drive/MyDrive/Capstone/bronze/\",\n",
        "                                     checkpoint_path=\"/content/drive/MyDrive/Capstone/checkpoint/bronze\"):\n",
        "    \"\"\"\n",
        "    Build the bronze layer for F1 data with incremental loading support.\n",
        "    Stores data in race-per-row format with proper partitioning.\n",
        "    \"\"\"\n",
        "    from datetime import datetime\n",
        "    from pyspark.sql.functions import lit, current_timestamp\n",
        "    import json\n",
        "    import time\n",
        "    import requests\n",
        "\n",
        "    # Checkpoint logic remains the same\n",
        "    current_year = datetime.now().year  # Get once at start\n",
        "    try:\n",
        "        checkpoint_df = spark.read.parquet(checkpoint_path)\n",
        "        last_checkpoint = checkpoint_df.orderBy(\"timestamp\", ascending=False).first()\n",
        "        last_processed_season = last_checkpoint.last_processed_season\n",
        "        print(f\"Last processed season from checkpoint: {last_processed_season}\")\n",
        "        seasons_to_process = list(range(last_processed_season + 1, current_year + 1))\n",
        "    except:\n",
        "        seasons_to_process = list(range(1950, current_year + 1))\n",
        "        print(\"No checkpoint found - processing all seasons from 1950\")\n",
        "\n",
        "    # Force include current season (even if already processed)\n",
        "    if current_year not in seasons_to_process:\n",
        "        seasons_to_process.append(current_year)\n",
        "        seasons_to_process = sorted(seasons_to_process)\n",
        "        print(f\"Added current season {current_year} to processing list\")\n",
        "\n",
        "    print(f\"Processing seasons: {seasons_to_process}\")\n",
        "\n",
        "    # Rate limiter implementation remains the same\n",
        "    class RateLimiter:\n",
        "        def __init__(self, burst_limit=4, hourly_limit=500):\n",
        "            self.burst_limit = burst_limit\n",
        "            self.hourly_limit = hourly_limit\n",
        "            self.request_timestamps = []\n",
        "\n",
        "        def wait_if_needed(self):\n",
        "            current_time = time.time()\n",
        "            self.request_timestamps = [ts for ts in self.request_timestamps if current_time - ts < 3600]\n",
        "\n",
        "            if len(self.request_timestamps) >= self.hourly_limit:\n",
        "                oldest = min(self.request_timestamps)\n",
        "                sleep = 3600 - (current_time - oldest) + 1\n",
        "                print(f\"Hourly limit reached. Sleeping {sleep:.1f}s\")\n",
        "                time.sleep(sleep)\n",
        "                return self.wait_if_needed()\n",
        "\n",
        "            recent = [ts for ts in self.request_timestamps if current_time - ts < 1]\n",
        "            if len(recent) >= self.burst_limit:\n",
        "                time.sleep(1)\n",
        "\n",
        "            self.request_timestamps.append(current_time)\n",
        "\n",
        "    rate_limiter = RateLimiter()\n",
        "\n",
        "    def make_api_request(url, params, retries=3):\n",
        "        for attempt in range(retries):\n",
        "            rate_limiter.wait_if_needed()\n",
        "            try:\n",
        "                response = requests.get(url, params=params)\n",
        "                if response.status_code == 200:\n",
        "                    return response.json()\n",
        "                elif response.status_code == 429:\n",
        "                    wait = min(30, (2 ** attempt) + 1)\n",
        "                    print(f\"Rate limited. Waiting {wait}s\")\n",
        "                    time.sleep(wait)\n",
        "                else:\n",
        "                    print(f\"HTTP {response.status_code}. Retry {attempt+1}/{retries}\")\n",
        "                    time.sleep(1)\n",
        "            except Exception as e:\n",
        "                print(f\"Error: {str(e)}. Retry {attempt+1}/{retries}\")\n",
        "                time.sleep(1)\n",
        "        return None\n",
        "\n",
        "    def fetch_season_races(season):\n",
        "        \"\"\"Fetch and enrich races for a season\"\"\"\n",
        "        print(f\"Fetching season {season}\")\n",
        "        url = f\"https://api.jolpi.ca/ergast/f1/{season}/results/\"\n",
        "\n",
        "        # Get initial count\n",
        "        initial = make_api_request(url, {\"limit\": 1, \"offset\": 0})\n",
        "        if not initial or \"MRData\" not in initial:\n",
        "            return None\n",
        "\n",
        "        total = int(initial[\"MRData\"][\"total\"])\n",
        "        limit = 100\n",
        "        all_races = []\n",
        "\n",
        "        for offset in range(0, total, limit):\n",
        "            result = make_api_request(url, {\"limit\": limit, \"offset\": offset})\n",
        "            if result and \"MRData\" in result and \"RaceTable\" in result[\"MRData\"]:\n",
        "                all_races.extend(result[\"MRData\"][\"RaceTable\"].get(\"Races\", []))\n",
        "\n",
        "        # Add metadata to each race\n",
        "        enriched = []\n",
        "        ingestion_ts = datetime.now().isoformat()\n",
        "        for race in all_races:\n",
        "            race[\"season\"] = season\n",
        "            race[\"source\"] = \"jolpica_api\"\n",
        "            race[\"ingestion_timestamp\"] = ingestion_ts\n",
        "            enriched.append(race)\n",
        "\n",
        "        return enriched\n",
        "\n",
        "    # Process seasons\n",
        "    for season in seasons_to_process:\n",
        "        print(f\"\\nProcessing season {season}\")\n",
        "        season_dir = f\"{bronze_path}/season={season}\"\n",
        "\n",
        "        # Handle current season differently\n",
        "        if season == current_year:\n",
        "            print(f\"Force refreshing current season {season}\")\n",
        "\n",
        "            # Delete existing directory if it exists\n",
        "            try:\n",
        "                hadoop_path = spark._jvm.org.apache.hadoop.fs.Path(season_dir)\n",
        "                fs = hadoop_path.getFileSystem(spark._jsc.hadoopConfiguration())\n",
        "                if fs.exists(hadoop_path):\n",
        "                    print(f\"Deleting existing data for season {season}\")\n",
        "                    fs.delete(hadoop_path, True)  # recursive delete\n",
        "            except Exception as e:\n",
        "                print(f\"Error deleting directory: {str(e)}\")\n",
        "                pass\n",
        "        else:\n",
        "            # Skip existing non-current seasons\n",
        "            try:\n",
        "                hadoop_path = spark._jvm.org.apache.hadoop.fs.Path(season_dir)\n",
        "                fs = hadoop_path.getFileSystem(spark._jsc.hadoopConfiguration())\n",
        "                if fs.exists(hadoop_path):\n",
        "                    print(f\"Skipping existing non-current season {season}\")\n",
        "                    continue\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Rest of processing logic remains unchanged...\n",
        "        # Fetch data\n",
        "        races = fetch_season_races(season)\n",
        "        if not races:\n",
        "            print(f\"No data for season {season}\")\n",
        "            continue\n",
        "\n",
        "        # Write using Spark\n",
        "        sc = spark.sparkContext\n",
        "        rdd = sc.parallelize([json.dumps(race) for race in races])\n",
        "        rdd.coalesce(1).saveAsTextFile(season_dir)  # Now safe to write\n",
        "\n",
        "        # Update checkpoint\n",
        "        checkpoint_data = [{\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"last_processed_season\": season,\n",
        "            \"records_processed\": len(races)\n",
        "        }]\n",
        "        spark.createDataFrame(checkpoint_data).write.mode(\"append\").parquet(checkpoint_path)\n",
        "\n",
        "        print(f\"Saved {len(races)} races for season {season}\")\n",
        "\n",
        "    print(\"Bronze layer update complete\")\n",
        "    return seasons_to_process\n",
        "\n",
        "spark = SparkSession.builder.appName(\"F1MedallionPipeline\").getOrCreate()\n",
        "fetch_f1_bronze_layer_incremental(\n",
        "        spark)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession, Window\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import types as T\n",
        "from datetime import datetime\n",
        "import json\n",
        "import hashlib\n",
        "import uuid\n",
        "\n",
        "class F1SilverLayer:\n",
        "    def __init__(self,\n",
        "                 spark,\n",
        "                 bronze_path=\"/content/drive/MyDrive/Capstone/bronze/\",\n",
        "                 silver_path=\"/content/drive/MyDrive/Capstone/silver/\",\n",
        "                 silver_checkpoint_path=\"/content/drive/MyDrive/Capstone/checkpoint/silver\"):\n",
        "        \"\"\"\n",
        "        Initialize the F1 Silver Layer processor\n",
        "        \"\"\"\n",
        "        self.spark = spark\n",
        "        self.bronze_path = bronze_path\n",
        "        self.silver_path = silver_path\n",
        "        self.silver_checkpoint_path = silver_checkpoint_path\n",
        "\n",
        "        # Configure Spark for optimal performance\n",
        "        self.spark.conf.set(\"spark.sql.parquet.compression.codec\", \"zstd\")\n",
        "        self.spark.conf.set(\"spark.sql.parquet.enableDictionaryEncoding\", \"true\")\n",
        "        self.spark.conf.set(\"spark.sql.parquet.block.size\", 256*1024*1024)\n",
        "\n",
        "        # Define schema for better control and performance\n",
        "        self.define_schemas()\n",
        "\n",
        "    def define_schemas(self):\n",
        "        \"\"\"Define explicit schemas for the silver layer\"\"\"\n",
        "        self.circuit_schema = T.StructType([\n",
        "            T.StructField(\"circuitId\", T.StringType(), True),\n",
        "            T.StructField(\"circuitName\", T.StringType(), True),\n",
        "            T.StructField(\"lat\", T.DoubleType(), True),\n",
        "            T.StructField(\"long\", T.DoubleType(), True),\n",
        "            T.StructField(\"locality\", T.StringType(), True),\n",
        "            T.StructField(\"country\", T.StringType(), True)\n",
        "        ])\n",
        "\n",
        "        self.result_schema = T.StructType([\n",
        "            T.StructField(\"constructorId\", T.StringType(), True),\n",
        "            T.StructField(\"constructorName\", T.StringType(), True),\n",
        "            T.StructField(\"driverId\", T.StringType(), True),\n",
        "            T.StructField(\"driverName\", T.StringType(), True),\n",
        "            T.StructField(\"position\", T.IntegerType(), True),\n",
        "            T.StructField(\"points\", T.DoubleType(), True),\n",
        "            T.StructField(\"grid\", T.IntegerType(), True),\n",
        "            T.StructField(\"laps\", T.IntegerType(), True),\n",
        "            T.StructField(\"status\", T.StringType(), True),\n",
        "            T.StructField(\"time\", T.StringType(), True)\n",
        "        ])\n",
        "\n",
        "    def get_last_processed_timestamp(self):\n",
        "        \"\"\"Get the last processed timestamp from checkpoint\"\"\"\n",
        "        try:\n",
        "            checkpoint_df = self.spark.read.parquet(self.silver_checkpoint_path)\n",
        "            last_checkpoint = checkpoint_df.orderBy(F.col(\"processed_timestamp\").desc()).first()\n",
        "            return last_checkpoint.processed_timestamp\n",
        "        except:\n",
        "            return \"1900-01-01T00:00:00\"\n",
        "\n",
        "    def process_bronze_data(self):\n",
        "        \"\"\"Process bronze data incrementally\"\"\"\n",
        "        # Read bronze data\n",
        "        bronze_df = self.spark.read.json(f\"{self.bronze_path}/season=*\")\n",
        "\n",
        "        # Get last processed timestamp\n",
        "        last_processed = self.get_last_processed_timestamp()\n",
        "\n",
        "        # Filter for new records\n",
        "        incremental_df = bronze_df.filter(\n",
        "            F.col(\"ingestion_timestamp\") > last_processed\n",
        "        )\n",
        "\n",
        "        if incremental_df.count() == 0:\n",
        "            print(\"No new data to process\")\n",
        "            return None\n",
        "\n",
        "        return self.transform_bronze_to_silver(incremental_df)\n",
        "\n",
        "    def transform_bronze_to_silver(self, df):\n",
        "        \"\"\"Transform bronze data into silver format with quality checks\"\"\"\n",
        "        # 1. Explode nested structures\n",
        "        df = df.select(\n",
        "            F.col(\"season\"),\n",
        "            F.col(\"round\"),\n",
        "            F.col(\"raceName\"),\n",
        "            F.col(\"date\"),\n",
        "            F.col(\"time\"),\n",
        "            F.to_timestamp(F.col(\"ingestion_timestamp\")).alias(\"ingestion_timestamp\"),\n",
        "            F.col(\"url\"),\n",
        "            F.col(\"Circuit\").alias(\"circuit\"),\n",
        "            F.explode(\"Results\").alias(\"result\")\n",
        "        )\n",
        "\n",
        "        # 2. Flatten nested structures with explicit casting\n",
        "        df = df.select(\n",
        "            \"*\",\n",
        "            F.col(\"circuit.circuitId\").alias(\"circuit_id\"),\n",
        "            F.col(\"circuit.circuitName\").alias(\"circuit_name\"),\n",
        "            F.col(\"circuit.Location.lat\").cast(\"double\").alias(\"circuit_lat\"),\n",
        "            F.col(\"circuit.Location.long\").cast(\"double\").alias(\"circuit_long\"),\n",
        "            F.col(\"circuit.Location.locality\").alias(\"circuit_locality\"),\n",
        "            F.col(\"circuit.Location.country\").alias(\"circuit_country\"),\n",
        "            F.col(\"result.Constructor.constructorId\").alias(\"constructor_id\"),\n",
        "            F.col(\"result.Constructor.name\").alias(\"constructor_name\"),\n",
        "            F.col(\"result.Driver.driverId\").alias(\"driver_id\"),\n",
        "            F.col(\"result.Driver.givenName\").alias(\"driver_given_name\"),\n",
        "            F.col(\"result.Driver.familyName\").alias(\"driver_family_name\"),\n",
        "            F.col(\"result.position\").cast(\"integer\").alias(\"position\"),\n",
        "            F.col(\"result.points\").cast(\"double\").alias(\"points\"),\n",
        "            F.col(\"result.grid\").cast(\"integer\").alias(\"grid\"),\n",
        "            F.col(\"result.laps\").cast(\"integer\").alias(\"laps\"),\n",
        "            F.col(\"result.status\").alias(\"status\"),\n",
        "            F.col(\"result.Time.time\").alias(\"finish_time\")\n",
        "        ).drop(\"circuit\", \"result\")\n",
        "\n",
        "        # 3. Data type conversions and standardization\n",
        "        df = df.withColumn(\"race_timestamp\",\n",
        "                          F.to_timestamp(\n",
        "                              F.concat(F.col(\"date\"), F.lit(\" \"), F.col(\"time\")),\n",
        "                              \"yyyy-MM-dd HH:mm:ssX\"\n",
        "                          ))\n",
        "\n",
        "        # 4. Add computed columns\n",
        "        df = df.withColumn(\"driver_full_name\",\n",
        "                          F.concat(F.col(\"driver_given_name\"), F.lit(\" \"), F.col(\"driver_family_name\")))\n",
        "\n",
        "        # 5. Generate hash key for change detection\n",
        "        columns_for_hash = [\"season\", \"round\", \"driver_id\", \"constructor_id\", \"position\"]\n",
        "        df = df.withColumn(\"row_hash\",\n",
        "                          F.sha2(F.concat_ws(\"|\", *[F.col(c) for c in columns_for_hash]), 256))\n",
        "\n",
        "        # 6. Add data quality columns\n",
        "        df = self.add_data_quality_checks(df)\n",
        "\n",
        "        # 7. Add metadata columns\n",
        "        df = df.withColumn(\"processed_timestamp\", F.current_timestamp())\n",
        "        df = df.withColumn(\"silver_batch_id\", F.lit(str(uuid.uuid4())))  # Using Python UUID instead of Spark UUID\n",
        "\n",
        "        return df\n",
        "\n",
        "    def add_data_quality_checks(self, df):\n",
        "        \"\"\"Add data quality check columns\"\"\"\n",
        "        # Define quality checks\n",
        "        df = df.withColumn(\"is_valid_position\",\n",
        "                          (F.col(\"position\").isNotNull() & (F.col(\"position\") >= 1)))\n",
        "\n",
        "        df = df.withColumn(\"is_valid_points\",\n",
        "                          (F.col(\"points\").isNotNull() & (F.col(\"points\") >= 0)))\n",
        "\n",
        "        df = df.withColumn(\"is_valid_grid\",\n",
        "                          (F.col(\"grid\").isNotNull() & (F.col(\"grid\") >= 0)))\n",
        "\n",
        "        df = df.withColumn(\"is_valid_date\",\n",
        "                          F.col(\"race_timestamp\").isNotNull())\n",
        "\n",
        "        # Combine all checks\n",
        "        df = df.withColumn(\"is_valid_record\",\n",
        "                          F.col(\"is_valid_position\") &\n",
        "                          F.col(\"is_valid_points\") &\n",
        "                          F.col(\"is_valid_grid\") &\n",
        "                          F.col(\"is_valid_date\"))\n",
        "\n",
        "        # Calculate null percentages\n",
        "        for column in df.columns:\n",
        "            df = df.withColumn(f\"is_null_{column}\",\n",
        "                             F.when(F.col(column).isNull(), 1).otherwise(0))\n",
        "\n",
        "        return df\n",
        "\n",
        "    def write_to_silver(self, df):\n",
        "        \"\"\"Write processed data to silver layer\"\"\"\n",
        "        if df is None or df.count() == 0:\n",
        "            return\n",
        "\n",
        "        # Write main silver table\n",
        "        (df.write\n",
        "         .mode(\"append\")\n",
        "         .partitionBy(\"season\")\n",
        "         .format(\"parquet\")\n",
        "         .option(\"compression\", \"zstd\")\n",
        "         .save(self.silver_path))\n",
        "\n",
        "        # Write quality metrics\n",
        "        quality_metrics = self.calculate_quality_metrics(df)\n",
        "        (quality_metrics.write\n",
        "         .mode(\"append\")\n",
        "         .format(\"parquet\")\n",
        "         .save(f\"{self.silver_path}_metrics\"))\n",
        "\n",
        "        # Update checkpoint\n",
        "        self.update_checkpoint(df)\n",
        "\n",
        "    def calculate_quality_metrics(self, df):\n",
        "        \"\"\"Calculate quality metrics for the batch\"\"\"\n",
        "        metrics = []\n",
        "\n",
        "        # Calculate null percentages\n",
        "        for column in df.columns:\n",
        "            null_count = df.filter(F.col(column).isNull()).count()\n",
        "            total_count = df.count()\n",
        "            null_percentage = (null_count / total_count) * 100 if total_count > 0 else 0\n",
        "            metrics.append({\n",
        "                \"metric_name\": f\"null_percentage_{column}\",\n",
        "                \"metric_value\": null_percentage,\n",
        "                \"batch_id\": df.select(F.first(\"silver_batch_id\")).collect()[0][0],\n",
        "                \"calculated_at\": datetime.now().isoformat()\n",
        "            })\n",
        "\n",
        "        # Add other metrics\n",
        "        metrics.extend([\n",
        "            {\n",
        "                \"metric_name\": \"total_records\",\n",
        "                \"metric_value\": df.count(),\n",
        "                \"batch_id\": df.select(F.first(\"silver_batch_id\")).collect()[0][0],\n",
        "                \"calculated_at\": datetime.now().isoformat()\n",
        "            },\n",
        "            {\n",
        "                \"metric_name\": \"invalid_records_percentage\",\n",
        "                \"metric_value\": (df.filter(~F.col(\"is_valid_record\")).count() / df.count()) * 100,\n",
        "                \"batch_id\": df.select(F.first(\"silver_batch_id\")).collect()[0][0],\n",
        "                \"calculated_at\": datetime.now().isoformat()\n",
        "            }\n",
        "        ])\n",
        "\n",
        "        return self.spark.createDataFrame(metrics)\n",
        "\n",
        "    def update_checkpoint(self, df):\n",
        "        \"\"\"Update the checkpoint with latest processed timestamp\"\"\"\n",
        "        checkpoint_data = [{\n",
        "            \"processed_timestamp\": df.select(F.max(\"ingestion_timestamp\")).collect()[0][0],\n",
        "            \"silver_batch_id\": df.select(F.first(\"silver_batch_id\")).collect()[0][0],\n",
        "            \"record_count\": df.count(),\n",
        "            \"checkpoint_timestamp\": datetime.now().isoformat()\n",
        "        }]\n",
        "\n",
        "        (self.spark.createDataFrame(checkpoint_data)\n",
        "         .write\n",
        "         .mode(\"append\")\n",
        "         .parquet(self.silver_checkpoint_path))\n",
        "\n",
        "    def process(self):\n",
        "        \"\"\"Main processing method\"\"\"\n",
        "        try:\n",
        "            print(\"Starting Silver Layer Processing...\")\n",
        "\n",
        "            # Process bronze to silver\n",
        "            silver_df = self.process_bronze_data()\n",
        "\n",
        "            # Write to silver if we have data\n",
        "            if silver_df is not None:\n",
        "                print(f\"Writing {silver_df.count()} records to silver layer...\")\n",
        "                self.write_to_silver(silver_df)\n",
        "                print(\"Silver Layer Processing Complete!\")\n",
        "            else:\n",
        "                print(\"No new data to process\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in Silver Layer Processing: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "# Usage\n",
        "if __name__ == \"__main__\":\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"F1SilverLayer\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    # Optional: Mount Google Drive if using Colab with Drive storage\n",
        "    # from google.colab import drive\n",
        "    # drive.mount('/content/drive')\n",
        "\n",
        "    silver_layer = F1SilverLayer(spark)\n",
        "    silver_layer.process()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "kd4glVLKdiJG",
        "outputId": "51e11d8b-8de8-47c9-d984-baa61efc3afd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Silver Layer Processing...\n",
            "Writing 25474 records to silver layer...\n",
            "Error in Silver Layer Processing: [CANNOT_MERGE_TYPE] Can not merge type `DoubleType` and `LongType`.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PySparkTypeError",
          "evalue": "[CANNOT_MERGE_TYPE] Can not merge type `DoubleType` and `LongType`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-dcf2b28a44be>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0msilver_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF1SilverLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0msilver_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-dcf2b28a44be>\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msilver_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Writing {silver_df.count()} records to silver layer...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_to_silver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msilver_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Silver Layer Processing Complete!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-dcf2b28a44be>\u001b[0m in \u001b[0;36mwrite_to_silver\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;31m# Write quality metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mquality_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_quality_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         (quality_metrics.write\n\u001b[1;32m    190\u001b[0m          \u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"append\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-dcf2b28a44be>\u001b[0m in \u001b[0;36mcalculate_quality_metrics\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    227\u001b[0m         ])\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1441\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m             )\n\u001b[0;32m-> 1443\u001b[0;31m         return self._create_dataframe(\n\u001b[0m\u001b[1;32m   1444\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1483\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1486\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchemaFromList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m             \u001b[0mtupled_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    953\u001b[0m         \u001b[0minfer_array_from_first_element\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacyInferArrayTypeFromFirstElement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mprefer_timestamp_ntz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_timestamp_ntz_preferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m         schema = reduce(\n\u001b[0m\u001b[1;32m    956\u001b[0m             \u001b[0m_merge_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m             (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_merge_type\u001b[0;34m(a, b, name)\u001b[0m\n\u001b[1;32m   1799\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStructType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1800\u001b[0m         \u001b[0mnfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStructType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1801\u001b[0;31m         fields = [\n\u001b[0m\u001b[1;32m   1802\u001b[0m             StructField(\n\u001b[1;32m   1803\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_merge_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNullType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1801\u001b[0m         fields = [\n\u001b[1;32m   1802\u001b[0m             StructField(\n\u001b[0;32m-> 1803\u001b[0;31m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_merge_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNullType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1804\u001b[0m             )\n\u001b[1;32m   1805\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_merge_type\u001b[0;34m(a, b, name)\u001b[0m\n\u001b[1;32m   1791\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m         \u001b[0;31m# TODO: type cast (such as int -> long)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1793\u001b[0;31m         raise PySparkTypeError(\n\u001b[0m\u001b[1;32m   1794\u001b[0m             \u001b[0merror_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"CANNOT_MERGE_TYPE\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1795\u001b[0m             \u001b[0mmessage_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"data_type1\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_type2\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPySparkTypeError\u001b[0m: [CANNOT_MERGE_TYPE] Can not merge type `DoubleType` and `LongType`."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.json('/content/sample_data/bronze/season=*')\n",
        "df.show(n=5)\n",
        "df.write.mode(\"overwrite\").parquet('/content/sample_data/silver/')"
      ],
      "metadata": {
        "id": "ni3pWMllKne9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the bronze data exists\n",
        "!ls -l /content/sample_data/bronze/"
      ],
      "metadata": {
        "id": "p1ClrKU5Lm5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/sample_data/silver/\n",
        "!mkdir -p /content/sample_data/silver_checkpoint/"
      ],
      "metadata": {
        "id": "0S_MM5FnPWx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bronze_df = spark.read.json('/content/sample_data/bronze/season=*')\n"
      ],
      "metadata": {
        "id": "HkItzs2GSOR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"F1MedallionPipeline\").getOrCreate()\n"
      ],
      "metadata": {
        "id": "CFiDHH65kNvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession, Window\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import types as T\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "# Initialize Spark Session (without Delta configurations)\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"F1SilverLayer\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# First, let's read the parquet files and see what we're working with\n",
        "def process_f1_silver_layer():\n",
        "    try:\n",
        "        print(\"Starting Silver Layer Processing...\")\n",
        "\n",
        "        # Read the parquet files\n",
        "        bronze_df = spark.read.json(\n",
        "            \"/content/drive/MyDrive/Capstone/bronze/season=*\"\n",
        "        )\n",
        "\n",
        "        print(\"Bronze data loaded. Starting transformations...\")\n",
        "\n",
        "        # 1. Explode nested structures\n",
        "        df = bronze_df.select(\n",
        "            F.col(\"season\"),\n",
        "            F.col(\"round\"),\n",
        "            F.col(\"raceName\"),\n",
        "            F.col(\"date\"),\n",
        "            F.col(\"time\"),\n",
        "            F.col(\"url\"),\n",
        "            F.col(\"Circuit\").alias(\"circuit\"),\n",
        "            F.explode(\"Results\").alias(\"result\")\n",
        "        )\n",
        "\n",
        "        # 2. Flatten nested structures\n",
        "        df = df.select(\n",
        "            \"*\",\n",
        "            F.col(\"circuit.circuitId\").alias(\"circuit_id\"),\n",
        "            F.col(\"circuit.circuitName\").alias(\"circuit_name\"),\n",
        "            F.col(\"circuit.Location.lat\").alias(\"circuit_lat\"),\n",
        "            F.col(\"circuit.Location.long\").alias(\"circuit_long\"),\n",
        "            F.col(\"circuit.Location.locality\").alias(\"circuit_locality\"),\n",
        "            F.col(\"circuit.Location.country\").alias(\"circuit_country\"),\n",
        "            F.col(\"result.Constructor.constructorId\").alias(\"constructor_id\"),\n",
        "            F.col(\"result.Constructor.name\").alias(\"constructor_name\"),\n",
        "            F.col(\"result.Driver.driverId\").alias(\"driver_id\"),\n",
        "            F.col(\"result.Driver.givenName\").alias(\"driver_given_name\"),\n",
        "            F.col(\"result.Driver.familyName\").alias(\"driver_family_name\"),\n",
        "            F.col(\"result.position\").alias(\"position\"),\n",
        "            F.col(\"result.points\").alias(\"points\"),\n",
        "            F.col(\"result.grid\").alias(\"grid\"),\n",
        "            F.col(\"result.laps\").alias(\"laps\"),\n",
        "            F.col(\"result.status\").alias(\"status\"),\n",
        "            F.col(\"result.Time.time\").alias(\"finish_time\")\n",
        "        ).drop(\"circuit\", \"result\")\n",
        "\n",
        "        # 3. Data type conversions and standardization\n",
        "        df = df.withColumn(\"race_timestamp\",\n",
        "                          F.to_timestamp(\n",
        "                              F.concat(F.col(\"date\"), F.lit(\" \"), F.col(\"time\")),\n",
        "                              \"yyyy-MM-dd HH:mm:ssX\"\n",
        "                          ))\n",
        "\n",
        "        # 4. Add computed columns\n",
        "        df = df.withColumn(\"driver_full_name\",\n",
        "                          F.concat(F.col(\"driver_given_name\"),\n",
        "                                 F.lit(\" \"),\n",
        "                                 F.col(\"driver_family_name\")))\n",
        "\n",
        "        # 5. Data quality checks\n",
        "        df = df.withColumn(\"is_valid_position\",\n",
        "                          (F.col(\"position\").isNotNull() &\n",
        "                           F.col(\"position\").cast(\"int\").isNotNull() &\n",
        "                           (F.col(\"position\").cast(\"int\") >= 1)))\n",
        "\n",
        "        df = df.withColumn(\"is_valid_points\",\n",
        "                          (F.col(\"points\").isNotNull() &\n",
        "                           F.col(\"points\").cast(\"double\").isNotNull() &\n",
        "                           (F.col(\"points\").cast(\"double\") >= 0)))\n",
        "\n",
        "        df = df.withColumn(\"is_valid_grid\",\n",
        "                          (F.col(\"grid\").isNotNull() &\n",
        "                           F.col(\"grid\").cast(\"int\").isNotNull() &\n",
        "                           (F.col(\"grid\").cast(\"int\") >= 0)))\n",
        "\n",
        "        df = df.withColumn(\"is_valid_date\",\n",
        "                          F.col(\"race_timestamp\").isNotNull())\n",
        "\n",
        "        # Combine all checks\n",
        "        df = df.withColumn(\"is_valid_record\",\n",
        "                          F.col(\"is_valid_position\") &\n",
        "                          F.col(\"is_valid_points\") &\n",
        "                          F.col(\"is_valid_grid\") &\n",
        "                          F.col(\"is_valid_date\"))\n",
        "\n",
        "        # 6. Add metadata\n",
        "        df = df.withColumn(\"processed_timestamp\", F.current_timestamp())\n",
        "\n",
        "        # 7. Calculate quality metrics\n",
        "        total_records = df.count()\n",
        "        valid_records = df.filter(F.col(\"is_valid_record\")).count()\n",
        "        null_percentages = {}\n",
        "\n",
        "        for column in df.columns:\n",
        "            null_count = df.filter(F.col(column).isNull()).count()\n",
        "            null_percentages[column] = (null_count / total_records) * 100\n",
        "\n",
        "        print(\"\\nData Quality Metrics:\")\n",
        "        print(f\"Total Records: {total_records}\")\n",
        "        print(f\"Valid Records: {valid_records}\")\n",
        "        print(f\"Invalid Records: {total_records - valid_records}\")\n",
        "        print(\"\\nNull Percentages:\")\n",
        "        for col, pct in null_percentages.items():\n",
        "            if pct > 0:\n",
        "                print(f\"{col}: {pct:.2f}%\")\n",
        "\n",
        "        # 8. Write to silver layer\n",
        "        print(\"\\nWriting to silver layer...\")\n",
        "\n",
        "        # Write main dataset\n",
        "        silver_path = \"/content/drive/MyDrive/Capstone/silver1\"\n",
        "        df.write.mode(\"overwrite\") \\\n",
        "            .partitionBy(\"season\") \\\n",
        "            .parquet(silver_path)\n",
        "\n",
        "        print(f\"\\nSilver layer data written to: {silver_path}\")\n",
        "        print(\"Silver Layer Processing Complete!\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in Silver Layer Processing: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute the processing\n",
        "silver_df = process_f1_silver_layer()\n",
        "\n",
        "# Show sample of the processed data\n",
        "print(\"\\nSample of processed data:\")\n",
        "silver_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEufzE5sQAxc",
        "outputId": "38138fba-1a3e-4840-eeb4-01591032b98d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Silver Layer Processing...\n",
            "Bronze data loaded. Starting transformations...\n",
            "\n",
            "Data Quality Metrics:\n",
            "Total Records: 25474\n",
            "Valid Records: 8366\n",
            "Invalid Records: 17108\n",
            "\n",
            "Null Percentages:\n",
            "time: 67.16%\n",
            "finish_time: 68.71%\n",
            "race_timestamp: 67.16%\n",
            "\n",
            "Writing to silver layer...\n",
            "\n",
            "Silver layer data written to: /content/drive/MyDrive/Capstone/silver1\n",
            "Silver Layer Processing Complete!\n",
            "\n",
            "Sample of processed data:\n",
            "+------+-----+------------------+----------+---------+--------------------+----------+--------------------+-----------+------------+----------------+---------------+--------------+----------------+--------------+-----------------+------------------+--------+------+----+----+--------+-----------+-------------------+----------------+-----------------+---------------+-------------+-------------+---------------+--------------------+\n",
            "|season|round|          raceName|      date|     time|                 url|circuit_id|        circuit_name|circuit_lat|circuit_long|circuit_locality|circuit_country|constructor_id|constructor_name|     driver_id|driver_given_name|driver_family_name|position|points|grid|laps|  status|finish_time|     race_timestamp|driver_full_name|is_valid_position|is_valid_points|is_valid_grid|is_valid_date|is_valid_record| processed_timestamp|\n",
            "+------+-----+------------------+----------+---------+--------------------+----------+--------------------+-----------+------------+----------------+---------------+--------------+----------------+--------------+-----------------+------------------+--------+------+----+----+--------+-----------+-------------------+----------------+-----------------+---------------+-------------+-------------+---------------+--------------------+\n",
            "|  2024|    1|Bahrain Grand Prix|2024-03-02|15:00:00Z|https://en.wikipe...|   bahrain|Bahrain Internati...|    26.0325|     50.5106|          Sakhir|        Bahrain|      red_bull|        Red Bull|max_verstappen|              Max|        Verstappen|       1|    26|   1|  57|Finished|1:31:44.742|2024-03-02 15:00:00|  Max Verstappen|             true|           true|         true|         true|           true|2025-04-16 07:50:...|\n",
            "|  2024|    1|Bahrain Grand Prix|2024-03-02|15:00:00Z|https://en.wikipe...|   bahrain|Bahrain Internati...|    26.0325|     50.5106|          Sakhir|        Bahrain|      red_bull|        Red Bull|         perez|           Sergio|             Pérez|       2|    18|   5|  57|Finished|    +22.457|2024-03-02 15:00:00|    Sergio Pérez|             true|           true|         true|         true|           true|2025-04-16 07:50:...|\n",
            "|  2024|    1|Bahrain Grand Prix|2024-03-02|15:00:00Z|https://en.wikipe...|   bahrain|Bahrain Internati...|    26.0325|     50.5106|          Sakhir|        Bahrain|       ferrari|         Ferrari|         sainz|           Carlos|             Sainz|       3|    15|   4|  57|Finished|    +25.110|2024-03-02 15:00:00|    Carlos Sainz|             true|           true|         true|         true|           true|2025-04-16 07:50:...|\n",
            "|  2024|    1|Bahrain Grand Prix|2024-03-02|15:00:00Z|https://en.wikipe...|   bahrain|Bahrain Internati...|    26.0325|     50.5106|          Sakhir|        Bahrain|       ferrari|         Ferrari|       leclerc|          Charles|           Leclerc|       4|    12|   2|  57|Finished|    +39.669|2024-03-02 15:00:00| Charles Leclerc|             true|           true|         true|         true|           true|2025-04-16 07:50:...|\n",
            "|  2024|    1|Bahrain Grand Prix|2024-03-02|15:00:00Z|https://en.wikipe...|   bahrain|Bahrain Internati...|    26.0325|     50.5106|          Sakhir|        Bahrain|      mercedes|        Mercedes|       russell|           George|           Russell|       5|    10|   3|  57|Finished|    +46.788|2024-03-02 15:00:00|  George Russell|             true|           true|         true|         true|           true|2025-04-16 07:50:...|\n",
            "+------+-----+------------------+----------+---------+--------------------+----------+--------------------+-----------+------------+----------------+---------------+--------------+----------------+--------------+-----------------+------------------+--------+------+----+----+--------+-----------+-------------------+----------------+-----------------+---------------+-------------+-------------+---------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.parquet('/content/drive/MyDrive/Capstone/silver/season=*')\n",
        "df.show(n=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "E8gjLa7YRuSZ",
        "outputId": "607245af-03fa-42d6-bc6b-5d1bf33c4905"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o275.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 109.0 failed 1 times, most recent failure: Lost task 0.0 in stage 109.0 (TID 517) (9d9453dfbdfb executor driver): org.apache.spark.SparkException: Parquet column cannot be converted in file file:///content/drive/MyDrive/Capstone/silver/season=2024/part-00000-b8276d09-4dc2-4133-872b-4ba68e15b3eb.c000.zstd.parquet. Column: [circuit_lat], Expected: string, Found: DOUBLE.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:855)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [circuit_lat], physicalType: DOUBLE, logicalType: string\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)\n\t... 23 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Parquet column cannot be converted in file file:///content/drive/MyDrive/Capstone/silver/season=2024/part-00000-b8276d09-4dc2-4133-872b-4ba68e15b3eb.c000.zstd.parquet. Column: [circuit_lat], Expected: string, Found: DOUBLE.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:855)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [circuit_lat], physicalType: DOUBLE, logicalType: string\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)\n\t... 23 more\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-fe23d5f278e5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Capstone/silver/season=*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    945\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mBob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m         \"\"\"\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     def _show_string(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o275.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 109.0 failed 1 times, most recent failure: Lost task 0.0 in stage 109.0 (TID 517) (9d9453dfbdfb executor driver): org.apache.spark.SparkException: Parquet column cannot be converted in file file:///content/drive/MyDrive/Capstone/silver/season=2024/part-00000-b8276d09-4dc2-4133-872b-4ba68e15b3eb.c000.zstd.parquet. Column: [circuit_lat], Expected: string, Found: DOUBLE.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:855)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [circuit_lat], physicalType: DOUBLE, logicalType: string\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)\n\t... 23 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Parquet column cannot be converted in file file:///content/drive/MyDrive/Capstone/silver/season=2024/part-00000-b8276d09-4dc2-4133-872b-4ba68e15b3eb.c000.zstd.parquet. Column: [circuit_lat], Expected: string, Found: DOUBLE.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:855)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [circuit_lat], physicalType: DOUBLE, logicalType: string\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)\n\t... 23 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.repartition(1).write.mode(\"overwrite\").csv('/content/sample_data/gold/')"
      ],
      "metadata": {
        "id": "TjKs-3riajzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession, Window\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import types as T\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "# Process F1 Silver Layer\n",
        "def process_f1_silver_layer():\n",
        "    try:\n",
        "        print(\"Starting Silver Layer Processing...\")\n",
        "\n",
        "        # Read the parquet file directly\n",
        "        file_path = \"/content/drive/MyDrive/Capstone/bronze/season=*\"\n",
        "\n",
        "        \"\"\"# First read with pandas to verify we can access the file\n",
        "        print(\"Reading file with pandas first to verify access...\")\n",
        "        pdf = pd.read_parquet(file_path)\n",
        "        print(f\"Successfully read with pandas: {len(pdf)} rows\")\"\"\"\n",
        "\n",
        "        # Now read with Spark\n",
        "        bronze_df = spark.read.parquet(file_path)\n",
        "\n",
        "        print(f\"Bronze data loaded with {bronze_df.count()} rows. Starting transformations...\")\n",
        "\n",
        "        # Display schema to understand the data\n",
        "        print(\"Bronze data schema:\")\n",
        "        bronze_df.printSchema()\n",
        "\n",
        "        # 1. Explode nested structures\n",
        "        df = bronze_df.select(\n",
        "            F.col(\"season\"),\n",
        "            F.col(\"round\"),\n",
        "            F.col(\"raceName\"),\n",
        "            F.col(\"date\"),\n",
        "            F.col(\"time\"),\n",
        "            F.col(\"url\"),\n",
        "            F.col(\"Circuit\").alias(\"circuit\"),\n",
        "            F.explode(\"Results\").alias(\"result\")\n",
        "        )\n",
        "\n",
        "        # 2. Flatten nested structures\n",
        "        df = df.select(\n",
        "            \"*\",\n",
        "            F.col(\"circuit.circuitId\").alias(\"circuit_id\"),\n",
        "            F.col(\"circuit.circuitName\").alias(\"circuit_name\"),\n",
        "            F.col(\"circuit.Location.lat\").alias(\"circuit_lat\"),\n",
        "            F.col(\"circuit.Location.long\").alias(\"circuit_long\"),\n",
        "            F.col(\"circuit.Location.locality\").alias(\"circuit_locality\"),\n",
        "            F.col(\"circuit.Location.country\").alias(\"circuit_country\"),\n",
        "            F.col(\"result.Constructor.constructorId\").alias(\"constructor_id\"),\n",
        "            F.col(\"result.Constructor.name\").alias(\"constructor_name\"),\n",
        "            F.col(\"result.Constructor.nationality\").alias(\"constructor_nationality\"),\n",
        "            F.col(\"result.Driver.driverId\").alias(\"driver_id\"),\n",
        "            F.col(\"result.Driver.givenName\").alias(\"driver_given_name\"),\n",
        "            F.col(\"result.Driver.familyName\").alias(\"driver_family_name\"),\n",
        "            F.col(\"result.Driver.nationality\").alias(\"driver_nationality\"),\n",
        "            F.col(\"result.Driver.dateOfBirth\").alias(\"driver_dob\"),\n",
        "            F.col(\"result.position\").alias(\"position\"),\n",
        "            F.col(\"result.points\").alias(\"points\"),\n",
        "            F.col(\"result.grid\").alias(\"grid\"),\n",
        "            F.col(\"result.laps\").alias(\"laps\"),\n",
        "            F.col(\"result.status\").alias(\"status\"),\n",
        "            F.col(\"result.Time.time\").alias(\"finish_time\")\n",
        "        ).drop(\"circuit\", \"result\")\n",
        "\n",
        "        # 3. Data type conversions and standardization\n",
        "        # Handle missing time values\n",
        "        df = df.withColumn(\"race_date\", F.to_date(F.col(\"date\"), \"yyyy-MM-dd\"))\n",
        "\n",
        "        # Handle race_timestamp conditionally since time might be null\n",
        "        df = df.withColumn(\"race_timestamp\",\n",
        "                          F.when(F.col(\"time\").isNotNull(),\n",
        "                                F.to_timestamp(F.concat(F.col(\"date\"), F.lit(\" \"), F.col(\"time\")),\n",
        "                                              \"yyyy-MM-dd HH:mm:ssX\"))\n",
        "                           .otherwise(F.to_timestamp(F.col(\"date\"), \"yyyy-MM-dd\")))\n",
        "\n",
        "        # Convert numeric columns to appropriate types\n",
        "        df = df.withColumn(\"position\", F.col(\"position\").cast(\"integer\"))\n",
        "        df = df.withColumn(\"points\", F.col(\"points\").cast(\"double\"))\n",
        "        df = df.withColumn(\"grid\", F.col(\"grid\").cast(\"integer\"))\n",
        "        df = df.withColumn(\"laps\", F.col(\"laps\").cast(\"integer\"))\n",
        "\n",
        "        # 4. Add computed columns\n",
        "        df = df.withColumn(\"driver_full_name\",\n",
        "                          F.concat(F.col(\"driver_given_name\"),\n",
        "                                 F.lit(\" \"),\n",
        "                                 F.col(\"driver_family_name\")))\n",
        "\n",
        "        # 5. Data quality checks\n",
        "        df = df.withColumn(\"is_valid_position\",\n",
        "                          (F.col(\"position\").isNotNull() &\n",
        "                           (F.col(\"position\") >= 1)))\n",
        "\n",
        "        df = df.withColumn(\"is_valid_points\",\n",
        "                          (F.col(\"points\").isNotNull() &\n",
        "                           (F.col(\"points\") >= 0)))\n",
        "\n",
        "        df = df.withColumn(\"is_valid_grid\",\n",
        "                          (F.col(\"grid\").isNotNull() &\n",
        "                           (F.col(\"grid\") >= 0)))\n",
        "\n",
        "        df = df.withColumn(\"is_valid_date\",\n",
        "                          F.col(\"race_date\").isNotNull())\n",
        "\n",
        "        # Combine all checks\n",
        "        df = df.withColumn(\"is_valid_record\",\n",
        "                          F.col(\"is_valid_position\") &\n",
        "                          F.col(\"is_valid_points\") &\n",
        "                          F.col(\"is_valid_grid\") &\n",
        "                          F.col(\"is_valid_date\"))\n",
        "\n",
        "        # 6. Add metadata\n",
        "        df = df.withColumn(\"processed_timestamp\", F.current_timestamp())\n",
        "\n",
        "        # 7. Calculate quality metrics\n",
        "        total_records = df.count()\n",
        "        valid_records = df.filter(F.col(\"is_valid_record\")).count()\n",
        "        null_percentages = {}\n",
        "\n",
        "        for column in df.columns:\n",
        "            null_count = df.filter(F.col(column).isNull()).count()\n",
        "            null_percentages[column] = (null_count / total_records) * 100\n",
        "\n",
        "        print(\"\\nData Quality Metrics:\")\n",
        "        print(f\"Total Records: {total_records}\")\n",
        "        print(f\"Valid Records: {valid_records}\")\n",
        "        print(f\"Invalid Records: {total_records - valid_records}\")\n",
        "        print(\"\\nNull Percentages:\")\n",
        "        for col, pct in null_percentages.items():\n",
        "            if pct > 0:\n",
        "                print(f\"{col}: {pct:.2f}%\")\n",
        "\n",
        "        # 8. Write to silver layer\n",
        "        print(\"\\nWriting to silver layer...\")\n",
        "\n",
        "        # Create output directory if it doesn't exist\n",
        "        output_dir = \"/content/drive/MyDrive/Capstone/silver1\"\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        # Write main dataset\n",
        "        silver_path = output_dir\n",
        "        df.write.mode(\"overwrite\") \\\n",
        "            .partitionBy(\"season\") \\\n",
        "            .parquet(silver_path)\n",
        "\n",
        "        print(f\"\\nSilver layer data written to: {silver_path}\")\n",
        "        print(\"Silver Layer Processing Complete!\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in Silver Layer Processing: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "\n",
        "# Execute the processing\n",
        "try:\n",
        "    silver_df = process_f1_silver_layer()\n",
        "\n",
        "    # Show sample of the processed data\n",
        "    print(\"\\nSample of processed data:\")\n",
        "    silver_df.show(5, truncate=False)\n",
        "\n",
        "    # Additional analysis - Top 5 drivers by points\n",
        "    print(\"\\nTop 5 drivers by points:\")\n",
        "    silver_df.groupBy(\"driver_full_name\") \\\n",
        "        .agg(F.sum(\"points\").alias(\"total_points\")) \\\n",
        "        .orderBy(F.col(\"total_points\").desc()) \\\n",
        "        .show(5)\n",
        "except Exception as e:\n",
        "    print(f\"Error in execution: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAd4IuJibF6z",
        "outputId": "0070e7fe-4b85-4789-9d3b-8b8dc2543015"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Silver Layer Processing...\n",
            "Error in Silver Layer Processing: An error occurred while calling o25.parquet.\n",
            ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 76) (9d9453dfbdfb executor driver): org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)\n",
            "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "Caused by: org.apache.spark.SparkException: [CANNOT_READ_FILE_FOOTER] Could not read footer for file: file:/content/drive/MyDrive/Capstone/bronze/season=1950/part-00000. Please ensure that the file is in either ORC or Parquet format. If not, please convert it to a valid format. If the file is in the valid format, please check if it is corrupt. If it is, you can choose to either ignore it or fix the corruption.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:1057)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:456)\n",
            "\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)\n",
            "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
            "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
            "\tat scala.util.Success.map(Try.scala:213)\n",
            "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
            "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n",
            "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n",
            "\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\n",
            "Caused by: java.lang.RuntimeException: file:/content/drive/MyDrive/Capstone/bronze/season=1950/part-00000 is not a Parquet file. Expected magic number at tail, but found [56, 34, 125, 10]\n",
            "\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:565)\n",
            "\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:799)\n",
            "\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)\n",
            "\t... 14 more\n",
            "\n",
            "Driver stacktrace:\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
            "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
            "\tat scala.Option.foreach(Option.scala:407)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
            "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
            "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
            "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n",
            "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.mergeSchemasInParallel(SchemaMergeUtils.scala:74)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:497)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.inferSchema(ParquetUtils.scala:132)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:79)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\n",
            "\tat scala.Option.orElse(Option.scala:447)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n",
            "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
            "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
            "\tat scala.Option.getOrElse(Option.scala:189)\n",
            "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
            "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
            "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)\n",
            "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\t... 1 more\n",
            "Caused by: org.apache.spark.SparkException: [CANNOT_READ_FILE_FOOTER] Could not read footer for file: file:/content/drive/MyDrive/Capstone/bronze/season=1950/part-00000. Please ensure that the file is in either ORC or Parquet format. If not, please convert it to a valid format. If the file is in the valid format, please check if it is corrupt. If it is, you can choose to either ignore it or fix the corruption.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:1057)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:456)\n",
            "\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)\n",
            "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
            "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
            "\tat scala.util.Success.map(Try.scala:213)\n",
            "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
            "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n",
            "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n",
            "\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\n",
            "Caused by: java.lang.RuntimeException: file:/content/drive/MyDrive/Capstone/bronze/season=1950/part-00000 is not a Parquet file. Expected magic number at tail, but found [56, 34, 125, 10]\n",
            "\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:565)\n",
            "\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:799)\n",
            "\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)\n",
            "\t... 14 more\n",
            "\n",
            "Error in execution: An error occurred while calling o25.parquet.\n",
            ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 76) (9d9453dfbdfb executor driver): org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)\n",
            "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "Caused by: org.apache.spark.SparkException: [CANNOT_READ_FILE_FOOTER] Could not read footer for file: file:/content/drive/MyDrive/Capstone/bronze/season=1950/part-00000. Please ensure that the file is in either ORC or Parquet format. If not, please convert it to a valid format. If the file is in the valid format, please check if it is corrupt. If it is, you can choose to either ignore it or fix the corruption.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:1057)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:456)\n",
            "\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)\n",
            "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
            "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
            "\tat scala.util.Success.map(Try.scala:213)\n",
            "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
            "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n",
            "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n",
            "\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\n",
            "Caused by: java.lang.RuntimeException: file:/content/drive/MyDrive/Capstone/bronze/season=1950/part-00000 is not a Parquet file. Expected magic number at tail, but found [56, 34, 125, 10]\n",
            "\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:565)\n",
            "\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:799)\n",
            "\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)\n",
            "\t... 14 more\n",
            "\n",
            "Driver stacktrace:\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
            "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
            "\tat scala.Option.foreach(Option.scala:407)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
            "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
            "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
            "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n",
            "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.mergeSchemasInParallel(SchemaMergeUtils.scala:74)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:497)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.inferSchema(ParquetUtils.scala:132)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:79)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\n",
            "\tat scala.Option.orElse(Option.scala:447)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n",
            "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
            "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
            "\tat scala.Option.getOrElse(Option.scala:189)\n",
            "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
            "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
            "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)\n",
            "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\t... 1 more\n",
            "Caused by: org.apache.spark.SparkException: [CANNOT_READ_FILE_FOOTER] Could not read footer for file: file:/content/drive/MyDrive/Capstone/bronze/season=1950/part-00000. Please ensure that the file is in either ORC or Parquet format. If not, please convert it to a valid format. If the file is in the valid format, please check if it is corrupt. If it is, you can choose to either ignore it or fix the corruption.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:1057)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:456)\n",
            "\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)\n",
            "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
            "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
            "\tat scala.util.Success.map(Try.scala:213)\n",
            "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
            "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n",
            "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n",
            "\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\n",
            "Caused by: java.lang.RuntimeException: file:/content/drive/MyDrive/Capstone/bronze/season=1950/part-00000 is not a Parquet file. Expected magic number at tail, but found [56, 34, 125, 10]\n",
            "\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:565)\n",
            "\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:799)\n",
            "\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)\n",
            "\t... 14 more\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-3-09f84070b798>\", line 23, in process_f1_silver_layer\n",
            "    bronze_df = spark.read.parquet(file_path)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py\", line 544, in parquet\n",
            "    return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
            "    return_value = get_return_value(\n",
            "                   ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
            "    return f(*a, **kw)\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\", line 326, in get_return_value\n",
            "    raise Py4JJavaError(\n",
            "py4j.protocol.Py4JJavaError: An error occurred while calling o25.parquet.\n",
            ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 76) (9d9453dfbdfb executor driver): org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)\n",
            "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "Caused by: org.apache.spark.SparkException: [CANNOT_READ_FILE_FOOTER] Could not read footer for file: file:/content/drive/MyDrive/Capstone/bronze/season=1950/part-00000. Please ensure that the file is in either ORC or Parquet format. If not, please convert it to a valid format. If the file is in the valid format, please check if it is corrupt. If it is, you can choose to either ignore it or fix the corruption.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:1057)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:456)\n",
            "\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)\n",
            "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
            "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
            "\tat scala.util.Success.map(Try.scala:213)\n",
            "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
            "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n",
            "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n",
            "\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\n",
            "Caused by: java.lang.RuntimeException: file:/content/drive/MyDrive/Capstone/bronze/season=1950/part-00000 is not a Parquet file. Expected magic number at tail, but found [56, 34, 125, 10]\n",
            "\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:565)\n",
            "\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:799)\n",
            "\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)\n",
            "\t... 14 more\n",
            "\n",
            "Driver stacktrace:\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
            "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
            "\tat scala.Option.foreach(Option.scala:407)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
            "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
            "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
            "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n",
            "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.mergeSchemasInParallel(SchemaMergeUtils.scala:74)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:497)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.inferSchema(ParquetUtils.scala:132)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:79)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\n",
            "\tat scala.Option.orElse(Option.scala:447)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n",
            "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
            "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
            "\tat scala.Option.getOrElse(Option.scala:189)\n",
            "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
            "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
            "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)\n",
            "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\t... 1 more\n",
            "Caused by: org.apache.spark.SparkException: [CANNOT_READ_FILE_FOOTER] Could not read footer for file: file:/content/drive/MyDrive/Capstone/bronze/season=1950/part-00000. Please ensure that the file is in either ORC or Parquet format. If not, please convert it to a valid format. If the file is in the valid format, please check if it is corrupt. If it is, you can choose to either ignore it or fix the corruption.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:1057)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:456)\n",
            "\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)\n",
            "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
            "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
            "\tat scala.util.Success.map(Try.scala:213)\n",
            "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
            "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n",
            "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n",
            "\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\n",
            "Caused by: java.lang.RuntimeException: file:/content/drive/MyDrive/Capstone/bronze/season=1950/part-00000 is not a Parquet file. Expected magic number at tail, but found [56, 34, 125, 10]\n",
            "\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:565)\n",
            "\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:799)\n",
            "\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)\n",
            "\t... 14 more\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-3-09f84070b798>\", line 162, in <cell line: 0>\n",
            "    silver_df = process_f1_silver_layer()\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-3-09f84070b798>\", line 23, in process_f1_silver_layer\n",
            "    bronze_df = spark.read.parquet(file_path)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py\", line 544, in parquet\n",
            "    return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
            "    return_value = get_return_value(\n",
            "                   ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
            "    return f(*a, **kw)\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\", line 326, in get_return_value\n",
            "    raise Py4JJavaError(\n",
            "py4j.protocol.Py4JJavaError: An error occurred while calling o25.parquet.\n",
            ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 76) (9d9453dfbdfb executor driver): org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)\n",
            "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "Caused by: org.apache.spark.SparkException: [CANNOT_READ_FILE_FOOTER] Could not read footer for file: file:/content/drive/MyDrive/Capstone/bronze/season=1950/part-00000. Please ensure that the file is in either ORC or Parquet format. If not, please convert it to a valid format. If the file is in the valid format, please check if it is corrupt. If it is, you can choose to either ignore it or fix the corruption.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:1057)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:456)\n",
            "\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)\n",
            "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
            "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
            "\tat scala.util.Success.map(Try.scala:213)\n",
            "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
            "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n",
            "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n",
            "\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\n",
            "Caused by: java.lang.RuntimeException: file:/content/drive/MyDrive/Capstone/bronze/season=1950/part-00000 is not a Parquet file. Expected magic number at tail, but found [56, 34, 125, 10]\n",
            "\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:565)\n",
            "\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:799)\n",
            "\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)\n",
            "\t... 14 more\n",
            "\n",
            "Driver stacktrace:\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
            "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
            "\tat scala.Option.foreach(Option.scala:407)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
            "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
            "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
            "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n",
            "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.mergeSchemasInParallel(SchemaMergeUtils.scala:74)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:497)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.inferSchema(ParquetUtils.scala:132)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:79)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\n",
            "\tat scala.Option.orElse(Option.scala:447)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n",
            "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
            "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
            "\tat scala.Option.getOrElse(Option.scala:189)\n",
            "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
            "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
            "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)\n",
            "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\t... 1 more\n",
            "Caused by: org.apache.spark.SparkException: [CANNOT_READ_FILE_FOOTER] Could not read footer for file: file:/content/drive/MyDrive/Capstone/bronze/season=1950/part-00000. Please ensure that the file is in either ORC or Parquet format. If not, please convert it to a valid format. If the file is in the valid format, please check if it is corrupt. If it is, you can choose to either ignore it or fix the corruption.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:1057)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:456)\n",
            "\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)\n",
            "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
            "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
            "\tat scala.util.Success.map(Try.scala:213)\n",
            "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
            "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n",
            "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n",
            "\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\n",
            "Caused by: java.lang.RuntimeException: file:/content/drive/MyDrive/Capstone/bronze/season=1950/part-00000 is not a Parquet file. Expected magic number at tail, but found [56, 34, 125, 10]\n",
            "\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:565)\n",
            "\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:799)\n",
            "\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)\n",
            "\t... 14 more\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fhmFOUUPkGuq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}